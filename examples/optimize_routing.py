#!/usr/bin/env python3
"""
dMC-Route Parameter Optimization using Jacobian Output

This script demonstrates gradient-based optimization of routing parameters
using dMC-Route's Jacobian output. It runs the model via subprocess and
uses the computed gradients to iteratively improve Manning's n values.

Usage:
    python optimize_routing.py -c config.txt -obs observations.csv

Requirements:
    - Compiled dMC-Route executable
    - Control file with all paths configured
    - Observed discharge CSV with columns: time, Q_obs
"""

import argparse
import subprocess
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Tuple, Optional
import shutil
import tempfile

try:
    import netCDF4 as nc
    HAS_NETCDF = True
except ImportError:
    HAS_NETCDF = False
    print("Warning: netCDF4 not installed. Cannot modify topology.nc parameters.")


def parse_control_file(filepath: str) -> Dict[str, str]:
    """Parse dMC-Route control file and extract tag values."""
    config = {}
    with open(filepath, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('!'):
                continue
            
            # Parse <tag> value format
            if '<' in line and '>' in line:
                tag_start = line.find('<')
                tag_end = line.find('>')
                tag = line[tag_start+1:tag_end]
                
                # Get value (everything after '>' until '!' or end)
                rest = line[tag_end+1:]
                if '!' in rest:
                    rest = rest[:rest.find('!')]
                value = rest.strip()
                
                if value:
                    config[tag] = value
    
    return config


def write_control_file(config: Dict[str, str], filepath: str):
    """Write a control file from config dictionary."""
    with open(filepath, 'w') as f:
        f.write("! Auto-generated dMC-Route control file\n")
        f.write("! Generated by optimize_routing.py\n\n")
        
        for tag, value in config.items():
            f.write(f"<{tag}>  {value}\n")


def load_observations(filepath: str, time_col: str = 'time', 
                      obs_col: str = 'Q_obs') -> Tuple[np.ndarray, np.ndarray]:
    """Load observed discharge from CSV."""
    df = pd.read_csv(filepath)
    
    # Try to find time and observation columns
    if time_col not in df.columns:
        # Try common alternatives
        for alt in ['Time', 'DATE', 'date', 'datetime']:
            if alt in df.columns:
                time_col = alt
                break
        else:
            # Use index as time
            times = np.arange(len(df))
            time_col = None
    
    if time_col:
        times = df[time_col].values
    
    if obs_col not in df.columns:
        # Try to find obs column
        for alt in ['obs', 'observed', 'Q', 'discharge', 'streamflow']:
            if alt in df.columns:
                obs_col = alt
                break
        else:
            raise ValueError(f"Cannot find observation column. Available: {list(df.columns)}")
    
    Q_obs = df[obs_col].values
    
    return times, Q_obs


def load_simulated(filepath: str, outlet_reach: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
    """Load simulated discharge from dMC-Route output CSV."""
    df = pd.read_csv(filepath)
    
    times = df['time'].values
    
    if outlet_reach is not None:
        col = f'Q_{outlet_reach}'
        if col in df.columns:
            Q_sim = df[col].values
        else:
            raise ValueError(f"Outlet reach {outlet_reach} not found in output")
    else:
        # Use last Q column (typically the outlet)
        q_cols = [c for c in df.columns if c.startswith('Q_')]
        if not q_cols:
            raise ValueError("No discharge columns found in output")
        Q_sim = df[q_cols[-1]].values
    
    return times, Q_sim


def load_jacobian(filepath: str) -> pd.DataFrame:
    """Load Jacobian matrix from CSV."""
    return pd.read_csv(filepath)


def get_manning_n_from_topology(topology_path: str) -> Dict[int, float]:
    """Read Manning's n values from topology.nc (if they exist)."""
    if not HAS_NETCDF:
        return {}
    
    manning_n = {}
    with nc.Dataset(topology_path, 'r') as ds:
        if 'segId' in ds.variables:
            seg_ids = ds.variables['segId'][:].astype(int)
            
            # Check if Manning's n is stored
            if 'mann_n' in ds.variables:
                n_values = ds.variables['mann_n'][:]
                for seg_id, n in zip(seg_ids, n_values):
                    manning_n[int(seg_id)] = float(n)
            else:
                # Use default
                for seg_id in seg_ids:
                    manning_n[int(seg_id)] = 0.03
    
    return manning_n


def set_manning_n_in_topology(topology_path: str, manning_n: Dict[int, float]):
    """Write Manning's n values to topology.nc."""
    if not HAS_NETCDF:
        raise RuntimeError("netCDF4 required to modify topology.nc")
    
    with nc.Dataset(topology_path, 'r+') as ds:
        seg_ids = ds.variables['segId'][:].astype(int)
        
        # Create mann_n variable if it doesn't exist
        if 'mann_n' not in ds.variables:
            ds.createVariable('mann_n', 'f8', ('seg',))
            ds.variables['mann_n'].long_name = "Manning roughness coefficient"
            ds.variables['mann_n'].units = "-"
        
        # Set values
        n_values = np.array([manning_n.get(int(sid), 0.03) for sid in seg_ids])
        ds.variables['mann_n'][:] = n_values


def compute_loss(Q_sim: np.ndarray, Q_obs: np.ndarray, 
                 loss_type: str = 'mse') -> float:
    """Compute loss between simulated and observed discharge."""
    # Handle length mismatch
    min_len = min(len(Q_sim), len(Q_obs))
    Q_sim = Q_sim[:min_len]
    Q_obs = Q_obs[:min_len]
    
    # Remove NaN values
    mask = ~(np.isnan(Q_sim) | np.isnan(Q_obs))
    Q_sim = Q_sim[mask]
    Q_obs = Q_obs[mask]
    
    if len(Q_sim) == 0:
        return float('inf')
    
    if loss_type == 'mse':
        return np.mean((Q_sim - Q_obs) ** 2)
    elif loss_type == 'rmse':
        return np.sqrt(np.mean((Q_sim - Q_obs) ** 2))
    elif loss_type == 'nse':
        # Return negative NSE (so we minimize)
        nse = 1 - np.sum((Q_sim - Q_obs) ** 2) / np.sum((Q_obs - np.mean(Q_obs)) ** 2)
        return -nse
    elif loss_type == 'kge':
        # Return negative KGE
        r = np.corrcoef(Q_sim, Q_obs)[0, 1]
        alpha = np.std(Q_sim) / np.std(Q_obs)
        beta = np.mean(Q_sim) / np.mean(Q_obs)
        kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)
        return -kge
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")


def compute_nse(Q_sim: np.ndarray, Q_obs: np.ndarray) -> float:
    """Compute Nash-Sutcliffe Efficiency."""
    min_len = min(len(Q_sim), len(Q_obs))
    Q_sim = Q_sim[:min_len]
    Q_obs = Q_obs[:min_len]
    
    return 1 - np.sum((Q_sim - Q_obs) ** 2) / np.sum((Q_obs - np.mean(Q_obs)) ** 2)


def compute_gradient_dL_dQ(Q_sim: np.ndarray, Q_obs: np.ndarray) -> float:
    """
    Compute dL/dQ for the loss function.
    
    For MSE loss: dL/dQ = 2 * (Q_sim - Q_obs) / N
    We sum over time to get scalar gradient at outlet.
    """
    min_len = min(len(Q_sim), len(Q_obs))
    Q_sim = Q_sim[:min_len]
    Q_obs = Q_obs[:min_len]
    
    # Sum of gradients over time (chain rule)
    return np.sum(2 * (Q_sim - Q_obs)) / len(Q_sim)


def run_dmc_route(executable: str, config_file: str, 
                  verbose: bool = False) -> subprocess.CompletedProcess:
    """Run dMC-Route executable with config file."""
    cmd = [executable, '-c', config_file]
    
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True
    )
    
    if verbose:
        print(result.stdout)
    
    if result.returncode != 0:
        print("STDERR:", result.stderr)
        raise RuntimeError(f"dMC-Route failed with code {result.returncode}")
    
    return result


class GradientDescentOptimizer:
    """Simple gradient descent with momentum."""
    
    def __init__(self, learning_rate: float = 0.001, momentum: float = 0.9):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocity = {}
    
    def step(self, params: Dict[int, float], grads: Dict[int, float]) -> Dict[int, float]:
        """Update parameters using gradients."""
        updated = {}
        
        for reach_id, param in params.items():
            grad = grads.get(reach_id, 0.0)
            
            # Initialize velocity
            if reach_id not in self.velocity:
                self.velocity[reach_id] = 0.0
            
            # Update with momentum
            self.velocity[reach_id] = self.momentum * self.velocity[reach_id] - self.lr * grad
            new_param = param + self.velocity[reach_id]
            
            # Clamp to valid range for Manning's n
            updated[reach_id] = np.clip(new_param, 0.01, 0.15)
        
        return updated


class AdamOptimizer:
    """Adam optimizer for parameter learning."""
    
    def __init__(self, learning_rate: float = 0.001, 
                 beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.t = 0
        self.m = {}
        self.v = {}
    
    def step(self, params: Dict[int, float], grads: Dict[int, float]) -> Dict[int, float]:
        """Update parameters using gradients."""
        self.t += 1
        updated = {}
        
        for reach_id, param in params.items():
            grad = grads.get(reach_id, 0.0)
            
            # Initialize moments
            if reach_id not in self.m:
                self.m[reach_id] = 0.0
                self.v[reach_id] = 0.0
            
            # Update moments
            self.m[reach_id] = self.beta1 * self.m[reach_id] + (1 - self.beta1) * grad
            self.v[reach_id] = self.beta2 * self.v[reach_id] + (1 - self.beta2) * (grad ** 2)
            
            # Bias correction
            m_hat = self.m[reach_id] / (1 - self.beta1 ** self.t)
            v_hat = self.v[reach_id] / (1 - self.beta2 ** self.t)
            
            # Update
            new_param = param - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
            
            # Clamp to valid range
            updated[reach_id] = np.clip(new_param, 0.01, 0.15)
        
        return updated


def optimize(config_file: str,
             obs_file: str,
             executable: str = './dmc_route_run',
             n_iterations: int = 50,
             learning_rate: float = 0.001,
             optimizer_type: str = 'adam',
             outlet_reach: Optional[int] = None,
             verbose: bool = True) -> Dict:
    """
    Run gradient-based optimization of Manning's n.
    
    Args:
        config_file: Path to dMC-Route control file
        obs_file: Path to observed discharge CSV
        executable: Path to dMC-Route executable
        n_iterations: Number of optimization iterations
        learning_rate: Learning rate for optimizer
        optimizer_type: 'adam' or 'sgd'
        outlet_reach: Outlet reach ID (auto-detect if None)
        verbose: Print progress
    
    Returns:
        Dictionary with optimization history
    """
    # Parse config
    config = parse_control_file(config_file)
    
    topology_file = config.get('fname_ntopo', config.get('network_file'))
    output_file = config.get('fname_output', 'discharge.csv')
    jacobian_file = config.get('fname_jacobian', 'jacobian.csv')
    
    if not topology_file:
        raise ValueError("Config must specify <fname_ntopo>")
    
    # Ensure gradients are enabled
    config['enable_gradients'] = 'true'
    config['fname_jacobian'] = jacobian_file
    
    # Load observations
    _, Q_obs = load_observations(obs_file)
    
    # Initialize Manning's n from topology or defaults
    manning_n = get_manning_n_from_topology(topology_file)
    if not manning_n:
        # Run once to get reach IDs from output
        temp_config = config_file + '.temp'
        write_control_file(config, temp_config)
        run_dmc_route(executable, temp_config, verbose=False)
        jac = load_jacobian(jacobian_file)
        manning_n = {int(rid): 0.03 for rid in jac['reach_id']}
        Path(temp_config).unlink()
    
    # Initialize optimizer
    if optimizer_type == 'adam':
        optimizer = AdamOptimizer(learning_rate=learning_rate)
    else:
        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)
    
    # History
    history = {
        'iteration': [],
        'loss': [],
        'nse': [],
        'manning_n_mean': [],
        'manning_n': []
    }
    
    # Create working directory
    work_dir = Path(tempfile.mkdtemp(prefix='dmc_opt_'))
    work_config = work_dir / 'config.txt'
    work_topology = work_dir / 'topology.nc'
    work_output = work_dir / 'discharge.csv'
    work_jacobian = work_dir / 'jacobian.csv'
    
    # Copy topology to working directory
    shutil.copy(topology_file, work_topology)
    
    # Update config for working directory
    config['fname_ntopo'] = str(work_topology)
    config['fname_output'] = str(work_output)
    config['fname_jacobian'] = str(work_jacobian)
    
    if verbose:
        print("=" * 70)
        print("dMC-Route Parameter Optimization")
        print("=" * 70)
        print(f"Topology:     {topology_file}")
        print(f"Observations: {obs_file}")
        print(f"Reaches:      {len(manning_n)}")
        print(f"Iterations:   {n_iterations}")
        print(f"Learning rate: {learning_rate}")
        print("=" * 70)
        print()
    
    try:
        for iteration in range(n_iterations):
            # Write current Manning's n to topology
            if HAS_NETCDF:
                set_manning_n_in_topology(str(work_topology), manning_n)
            
            # Write config
            write_control_file(config, str(work_config))
            
            # Run model
            run_dmc_route(executable, str(work_config), verbose=False)
            
            # Load results
            _, Q_sim = load_simulated(str(work_output), outlet_reach)
            jac = load_jacobian(str(work_jacobian))
            
            # Compute loss and metrics
            loss = compute_loss(Q_sim, Q_obs, 'mse')
            nse = compute_nse(Q_sim, Q_obs)
            
            # Compute dL/dQ (scalar for outlet)
            dL_dQ = compute_gradient_dL_dQ(Q_sim, Q_obs)
            
            # Chain rule: dL/dn = dL/dQ * dQ/dn
            # dQ/dn comes from Jacobian
            grads = {}
            for _, row in jac.iterrows():
                reach_id = int(row['reach_id'])
                dQ_dn = row['manning_n']  # From Jacobian
                grads[reach_id] = dL_dQ * dQ_dn
            
            # Update parameters
            manning_n = optimizer.step(manning_n, grads)
            
            # Record history
            history['iteration'].append(iteration)
            history['loss'].append(loss)
            history['nse'].append(nse)
            history['manning_n_mean'].append(np.mean(list(manning_n.values())))
            history['manning_n'].append(manning_n.copy())
            
            if verbose and (iteration % 5 == 0 or iteration == n_iterations - 1):
                n_mean = np.mean(list(manning_n.values()))
                print(f"Iter {iteration:3d}: Loss = {loss:.6f}, NSE = {nse:.4f}, mean(n) = {n_mean:.4f}")
        
        if verbose:
            print()
            print("=" * 70)
            print("Optimization Complete")
            print("=" * 70)
            print(f"Final NSE:     {history['nse'][-1]:.4f}")
            print(f"Final Loss:    {history['loss'][-1]:.6f}")
            print(f"Mean Manning n: {history['manning_n_mean'][-1]:.4f}")
            print()
            
            # Print final parameters
            print("Final Manning's n by reach:")
            for reach_id, n in sorted(manning_n.items()):
                print(f"  Reach {reach_id}: {n:.4f}")
    
    finally:
        # Cleanup
        shutil.rmtree(work_dir, ignore_errors=True)
    
    return history


def plot_optimization(history: Dict, output_file: str = 'optimization_results.png'):
    """Plot optimization history."""
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("matplotlib not available for plotting")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # Loss
    ax = axes[0, 0]
    ax.semilogy(history['iteration'], history['loss'])
    ax.set_xlabel('Iteration')
    ax.set_ylabel('MSE Loss')
    ax.set_title('Training Loss')
    ax.grid(True)
    
    # NSE
    ax = axes[0, 1]
    ax.plot(history['iteration'], history['nse'])
    ax.axhline(0, color='gray', linestyle=':', alpha=0.5)
    ax.axhline(1, color='green', linestyle='--', alpha=0.5)
    ax.set_xlabel('Iteration')
    ax.set_ylabel('NSE')
    ax.set_title('Nash-Sutcliffe Efficiency')
    ax.grid(True)
    
    # Mean Manning's n
    ax = axes[1, 0]
    ax.plot(history['iteration'], history['manning_n_mean'])
    ax.set_xlabel('Iteration')
    ax.set_ylabel("Mean Manning's n")
    ax.set_title('Parameter Convergence')
    ax.grid(True)
    
    # All Manning's n values
    ax = axes[1, 1]
    all_n = np.array([[n for n in h.values()] for h in history['manning_n']])
    for i in range(all_n.shape[1]):
        ax.plot(history['iteration'], all_n[:, i], alpha=0.5, linewidth=0.5)
    ax.plot(history['iteration'], history['manning_n_mean'], 'k-', linewidth=2, label='Mean')
    ax.set_xlabel('Iteration')
    ax.set_ylabel("Manning's n")
    ax.set_title("All Reach Parameters")
    ax.legend()
    ax.grid(True)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=150)
    print(f"\nResults saved to {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='Optimize dMC-Route Manning\'s n using gradients',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s -c config.txt -obs observations.csv
  %(prog)s -c config.txt -obs obs.csv -n 100 -lr 0.005
  %(prog)s -c config.txt -obs obs.csv --exe /path/to/dmc_route_run
        """
    )
    
    parser.add_argument('-c', '--config', required=True,
                        help='dMC-Route control file')
    parser.add_argument('-obs', '--observations', required=True,
                        help='Observed discharge CSV file')
    parser.add_argument('--exe', default='./dmc_route_run',
                        help='Path to dMC-Route executable')
    parser.add_argument('-n', '--iterations', type=int, default=50,
                        help='Number of iterations (default: 50)')
    parser.add_argument('-lr', '--learning-rate', type=float, default=0.001,
                        help='Learning rate (default: 0.001)')
    parser.add_argument('--optimizer', choices=['adam', 'sgd'], default='adam',
                        help='Optimizer type (default: adam)')
    parser.add_argument('--outlet', type=int, default=None,
                        help='Outlet reach ID (auto-detect if not specified)')
    parser.add_argument('--plot', action='store_true',
                        help='Generate optimization plots')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='Suppress progress output')
    
    args = parser.parse_args()
    
    # Run optimization
    history = optimize(
        config_file=args.config,
        obs_file=args.observations,
        executable=args.exe,
        n_iterations=args.iterations,
        learning_rate=args.learning_rate,
        optimizer_type=args.optimizer,
        outlet_reach=args.outlet,
        verbose=not args.quiet
    )
    
    # Plot if requested
    if args.plot:
        plot_optimization(history)
    
    # Save history
    df = pd.DataFrame({
        'iteration': history['iteration'],
        'loss': history['loss'],
        'nse': history['nse'],
        'manning_n_mean': history['manning_n_mean']
    })
    df.to_csv('optimization_history.csv', index=False)
    print("History saved to optimization_history.csv")


if __name__ == '__main__':
    main()
